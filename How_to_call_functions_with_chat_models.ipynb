{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1rVq4aG3rE53isvWdnM06BLK-Y0wTxgDH",
      "authorship_tag": "ABX9TyOjBSc3FmlBVR0eY/3cv3DE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaitas/ChatGPT-Colab/blob/main/How_to_call_functions_with_chat_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenGPTチャットモデルで関数を呼び出す方法\n",
        "このノートでは、GPTモデルの機能を拡張するために、チャット補完APIを外部関数と組み合わせて使用する方法について説明します。\n",
        "\n",
        "[original]\n",
        "https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb\n",
        "\n",
        "## functionsの使用方法\n",
        "functionsはChatCompletion APIのオプションパラメータで、関数の仕様を提供するために使用することができます。この目的は、モデルが関数入力スキーマに準拠した出力を生成することを可能にすることです。なお、APIは実際に関数の呼び出しを実行することはありません。モデルの出力を使って関数呼び出しを実行するのは、開発者次第です。\n",
        "\n",
        "functionsパラメータが提供されている場合、デフォルトでは、モデルは関数のいずれかを使用することが適切である場合に決定します。また、function_callパラメータに{\"name\"}を設定することで、特定の関数を使用するようにAPIを強制することもできます： \"<insert-function-name>\"}とすることで、特定の関数を強制的に使用させることも可能です。関数が使用された場合、出力には \"finish_reason \"が含まれます： \"function_call \"と、関数名と生成された関数引数を持つfunction_callオブジェクトがレスポンスに含まれます。\n",
        "\n",
        "関数は、以下のフィールドで指定します：\n",
        "\n",
        "- `Name`： Name：関数の名前。\n",
        "- `Description`： 関数が何をするのかの説明。モデルはこれを用いて、関数を呼び出すタイミングを決定します。\n",
        "- `Parameters`: パラメータ： parametersオブジェクトは、関数が必要とするすべての入力フィールドを含んでいます。\n",
        "これらの入力は、次のようなタイプになります： String、Number、Boolean、Object、Null、AnyOf。詳細については、[APIリファレンスドキュメント](https://platform.openai.com/docs/api-reference/chat)を参照してください。\n",
        "- `Required`（必須）： クエリを作成するために必要なパラメータを指定します。残りはオプションとして扱われます。\n",
        "\n",
        "関数を実行し、関数実行の出力をアシスタントに直接戻すことで、関数呼び出しを連鎖させることができます。これは、モデルが無限に関数を呼び続ける無限ループの挙動につながる可能性がありますが、これを防ぐためにガードレールを設置することができます。\n",
        "\n",
        "# ウォークスルー\n",
        "\n",
        "このクックブックでは、以下のワークフローを説明します：\n",
        "\n",
        "- 基本的な概念： 基本的なコンセプト：サンプル関数を作成し、必要に応じてAPIにそれを使用させる。\n",
        "- APIコールを関数実行に統合する： APIコールを使用して関数の引数を生成し、関数を実行するエージェントを作成する。\n",
        "- 複数の関数を使用する： ユーザーに応答する前に、複数の関数を順番に呼び出すことができるようにする。"
      ],
      "metadata": {
        "id": "IuUGSYvuadOs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prM2HKjzaY0f",
        "outputId": "1d535bf0-5798-4bfa-cb1c-4694fc5a7df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.10)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser->arxiv) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install scipy\n",
        "!pip install tenacity\n",
        "!pip install tiktoken\n",
        "!pip install termcolor\n",
        "!pip install openai\n",
        "!pip install requests\n",
        "!pip install arxiv\n",
        "!pip install pandas\n",
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import ast\n",
        "import concurrent\n",
        "from csv import writer\n",
        "from IPython.display import display, Markdown, Latex\n",
        "import json\n",
        "import openai\n",
        "import os\n",
        "import pandas as pd\n",
        "from PyPDF2 import PdfReader\n",
        "import requests\n",
        "from scipy import spatial\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n"
      ],
      "metadata": {
        "id": "WVd8Jxtpaw49"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 自分はGCPのSecretManagerサービスを使ってカギを保存しています\n",
        "!pip install google-cloud-secret-manager\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.cloud import secretmanager\n",
        "\n",
        "def access_secret(project_id, secret_name, version='latest'):\n",
        "    client = secretmanager.SecretManagerServiceClient()\n",
        "    name = client.secret_version_path(project_id, secret_name, version)\n",
        "    response = client.access_secret_version(request={\"name\":name})\n",
        "    payload = response.payload.data.decode(\"UTF-8\")\n",
        "    return payload\n",
        "\n",
        "openai.api_key = access_secret(\"awfree-aki2021\", \"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEjgYkBsdmSE",
        "outputId": "e86199f6-14f8-4c97-fe28-d72bde7d1412"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-cloud-secret-manager in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-secret-manager) (2.11.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-secret-manager) (1.22.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-secret-manager) (3.20.3)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-secret-manager) (0.12.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (2.27.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (1.54.0)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (4.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (3.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager) (0.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ユーティリティ\n",
        "まず、Chat Completions APIを呼び出したり、会話の状態を維持・管理するためのユーティリティをいくつか定義しておきましょう。"
      ],
      "metadata": {
        "id": "0KneHz6Ba-kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def chat_completion_request(messages, functions=None, model=GPT_MODEL):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
        "    }\n",
        "    json_data = {\"model\": model, \"messages\": messages}\n",
        "    if functions is not None:\n",
        "        json_data.update({\"functions\": functions})\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://api.openai.com/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=json_data,\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(\"Unable to generate ChatCompletion response\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return e"
      ],
      "metadata": {
        "id": "q9oX5h4GbBbU"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conversation:\n",
        "    def __init__(self):\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        message = {\"role\": role, \"content\": content}\n",
        "        self.conversation_history.append(message)\n",
        "\n",
        "    def display_conversation(self, detailed=False):\n",
        "        role_to_color = {\n",
        "            \"system\": \"red\",\n",
        "            \"user\": \"green\",\n",
        "            \"assistant\": \"blue\",\n",
        "            \"function\": \"magenta\",\n",
        "        }\n",
        "        for message in self.conversation_history:\n",
        "            print(\n",
        "                colored(\n",
        "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
        "                    role_to_color[message[\"role\"]],\n",
        "                )\n",
        "            )"
      ],
      "metadata": {
        "id": "I9Tm_0MEbv31"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 基本的な考え方\n",
        "次に、`get_current_weather`という関数の仕様書を作成します。後で、この関数仕様をAPIに渡して、仕様に沿った関数引数を生成するようにします。"
      ],
      "metadata": {
        "id": "Aw0wPbV3b1X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "    {\n",
        "        \"name\": \"get_current_weather\",\n",
        "        \"description\": \"現在の天気を取得する\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"都市と州（例：San Francisco, CA\",\n",
        "                },\n",
        "                \"format\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                    \"description\": \"使用する温度の単位です。ユーザーの位置情報から推測されます。\",\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"location\", \"format\"],\n",
        "        },\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "9hJg8jORb6R8"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = Conversation()\n",
        "conversation.add_message(\"user\", \"きょうはどんな天気？\")"
      ],
      "metadata": {
        "id": "I8OqgqfNcA_c"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルはまず、天気予報の機能を使うために必要な情報をユーザーに要求する\n",
        "chat_response = chat_completion_request(\n",
        "    conversation.conversation_history, functions=functions\n",
        ")\n",
        "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
        "conversation.add_message(assistant_message[\"role\"], assistant_message[\"content\"])\n",
        "assistant_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3ZXpasjcGnN",
        "outputId": "9b8e09e7-fd56-456d-903a-1d9254f9372d"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'role': 'assistant', 'content': 'どこの地域の天気情報が知りたいですか？'}"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ユーザーが必要な情報を提供すると、モデルは関数の引数を生成することができます。\n",
        "conversation.add_message(\"user\", \"日本の横浜に住んでます\")\n",
        "chat_response = chat_completion_request(\n",
        "    conversation.conversation_history, functions=functions\n",
        ")\n",
        "chat_response.json()[\"choices\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY3sgIwgeWlV",
        "outputId": "b013a239-bd73-45ed-e520-4147db1eedd7"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'index': 0,\n",
              " 'message': {'role': 'assistant',\n",
              "  'content': None,\n",
              "  'function_call': {'name': 'get_current_weather',\n",
              "   'arguments': '{\\n  \"location\": \"Yokohama, Japan\",\\n  \"format\": \"celsius\"\\n}'}},\n",
              " 'finish_reason': 'function_call'}"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑関数 `get_current_weather`のファンクションコールと引数を表現するJSONが生成されました。\n",
        "\n",
        "★ここまでJSONで出力せよとは指定していないことに注目。"
      ],
      "metadata": {
        "id": "Ch9Dt_XCiJ4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APIコールと関数実行を統合する\n",
        "次の例では、入力がモデルで生成された関数を実行する方法を示し、これを使用して、データベースに関する質問に答えるエージェントを実装します。簡単のために、Chinookサンプルデータベースを使用します。\n",
        "\n",
        "注：SQL生成のユースケースは、本番環境ではリスクが高いです - モデルは一貫したSQL構文を生成する際に信頼できないことがあります。この問題を解決する、より信頼性の高い方法は、モデルから目的のカラムを入力として受け取るクエリ生成APIを構築することかもしれません。\n",
        "\n",
        "### SQLデータベース情報を引き出す\n",
        "まず、SQLiteデータベースからデータを抽出するための便利なユーティリティ関数をいくつか定義しましょう。"
      ],
      "metadata": {
        "id": "Uk7rnhDHfSyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "# 公式からDBファイルを入手 https://github.com/openai/openai-cookbook/blob/main/examples/data/Chinook.db\n",
        "conn = sqlite3.connect(\"sample_data/Chinook.db\")\n",
        "print(\"データベースを正常に開きました\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-4dFBN3fWqe",
        "outputId": "c115fe2c-c1d5-498d-8462-769dd7b981b6"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "データベースを正常に開きました\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_table_names(conn):\n",
        "    \"\"\"Return a list of table names.\"\"\"\n",
        "    table_names = []\n",
        "    tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "    for table in tables.fetchall():\n",
        "        table_names.append(table[0])\n",
        "    return table_names\n",
        "\n",
        "\n",
        "def get_column_names(conn, table_name):\n",
        "    \"\"\"Return a list of column names.\"\"\"\n",
        "    column_names = []\n",
        "    columns = conn.execute(f\"PRAGMA table_info('{table_name}');\").fetchall()\n",
        "    for col in columns:\n",
        "        column_names.append(col[1])\n",
        "    return column_names\n",
        "\n",
        "\n",
        "def get_database_info(conn):\n",
        "    \"\"\"Return a list of dicts containing the table name and columns for each table in the database.\"\"\"\n",
        "    table_dicts = []\n",
        "    for table_name in get_table_names(conn):\n",
        "        columns_names = get_column_names(conn, table_name)\n",
        "        table_dicts.append({\"table_name\": table_name, \"column_names\": columns_names})\n",
        "    return table_dicts"
      ],
      "metadata": {
        "id": "W_1dN2Ndfd51"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "これらのユーティリティ関数を使用して、データベーススキーマの表現を抽出することができます。\n"
      ],
      "metadata": {
        "id": "zHKYtB3nf8km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "database_schema_dict = get_database_info(conn)\n",
        "database_schema_string = \"\\n\".join(\n",
        "    [\n",
        "        f\"Table: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\"\n",
        "        for table in database_schema_dict\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "YlnT2O42f4at"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "前回と同様に、APIに引数を生成させる関数の仕様を定義します。データベース・スキーマを関数仕様に挿入していることに注目してください。これは、モデルが知っておくべき重要なことです。"
      ],
      "metadata": {
        "id": "B8IanlOUf5VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "    {\n",
        "        \"name\": \"ask_database\",\n",
        "        \"description\": \"Use this function to answer user questions about music. Input should be a fully formed SQL query.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": f\"\"\"\n",
        "                            SQL query extracting info to answer the user's question.\n",
        "                            SQL should be written using this database schema:\n",
        "                            {database_schema_string}\n",
        "                            The query should be returned in plain text, not in JSON.\n",
        "                            \"\"\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"],\n",
        "        },\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "StSpggn6gFvO"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQLの実行\n",
        "では、エージェントがデータベースへの問い合わせに使用する関数を実装しましょう。また、Chat Completions APIへの呼び出しと、それが呼び出される関数を統合するためのユーティリティを実装する必要があります。\n"
      ],
      "metadata": {
        "id": "3ITa9FPbgYuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_database(conn, query):\n",
        "    \"\"\"指定されたSQLクエリでSQLiteデータベースに問い合わせを行う関数です。\"\"\"\n",
        "    try:\n",
        "        results = conn.execute(query).fetchall()\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"SQL error: {e}\")\n",
        "\n",
        "\n",
        "def chat_completion_with_function_execution(messages, functions=None):\n",
        "    \"\"\"この関数は、ChatCompletion APIコールを行い、関数コールが要求された場合は、その関数を実行する。\"\"\"\n",
        "    try:\n",
        "        response = chat_completion_request(messages, functions)\n",
        "        full_message = response.json()[\"choices\"][0]\n",
        "        if full_message[\"finish_reason\"] == \"function_call\":\n",
        "            print(f\"Function generation requested, calling function\")\n",
        "            return call_function(messages, full_message)\n",
        "        else:\n",
        "            print(f\"Function not required, responding to user\")\n",
        "            return response.json()\n",
        "    except Exception as e:\n",
        "        print(\"Unable to generate ChatCompletion response\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return response\n",
        "\n",
        "\n",
        "def call_function(messages, full_message):\n",
        "    \"\"\"モデルで生成された関数引数を用いて、関数呼び出しを実行する。\"\"\"\n",
        "\n",
        "    # We'll add our one function here - this can be extended with any additional functions\n",
        "    if full_message[\"message\"][\"function_call\"][\"name\"] == \"ask_database\":\n",
        "        query = eval(full_message[\"message\"][\"function_call\"][\"arguments\"])\n",
        "        print(f\"Prepped query is {query}\")\n",
        "        try:\n",
        "            results = ask_database(conn, query[\"query\"])\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "            # この次のブロックは、後続の呼び出しでクエリ生成の問題を修正しようとするものです。\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": f\"\"\"Query: {query['query']}\n",
        "The previous query received the error {e}.\n",
        "Please return a fixed SQL query in plain text.\n",
        "Your response should consist of ONLY the SQL query with the separator sql_start at the beginning and sql_end at the end\"\"\",\n",
        "                }\n",
        "            )\n",
        "            response = chat_completion_request(messages, model=\"gpt-4-0613\")\n",
        "\n",
        "            # 修正したSQLクエリで再試行する。2回目に失敗した場合は終了します。\n",
        "            try:\n",
        "                cleaned_query = response.json()[\"choices\"][0][\"message\"][\n",
        "                    \"content\"\n",
        "                ].split(\"sql_start\")[1]\n",
        "                cleaned_query = cleaned_query.split(\"sql_end\")[0]\n",
        "                print(cleaned_query)\n",
        "                results = ask_database(conn, cleaned_query)\n",
        "                print(results)\n",
        "                print(\"Got on second try\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"Second failure, exiting\")\n",
        "\n",
        "                print(f\"Function execution failed\")\n",
        "                print(f\"Error message: {e}\")\n",
        "\n",
        "        messages.append(\n",
        "            {\"role\": \"function\", \"name\": \"ask_database\", \"content\": str(results)}\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = chat_completion_request(messages)\n",
        "            return response.json()\n",
        "        except Exception as e:\n",
        "            print(type(e))\n",
        "            print(e)\n",
        "            raise Exception(\"ファンクションチャットのリクエストに失敗しました\")\n",
        "    else:\n",
        "        raise Exception(\"機能が存在しないため、呼び出すことができない\")\n"
      ],
      "metadata": {
        "id": "9NYpcjaMf7xe"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_system_message = \"\"\"あなたはChinookGPT、Chinook Music Databaseからユーザーの質問に対する回答を得る、親切なアシスタントです。\n",
        "できるだけ多くの情報をユーザーに提供すること。\n",
        "さあ開始！\"\"\"\n",
        "\n",
        "sql_conversation = Conversation()\n",
        "sql_conversation.add_message(\"system\", agent_system_message)\n",
        "sql_conversation.add_message(\n",
        "    \"user\", \"こんにちは、楽曲数の多いアーティストトップ5は誰ですか？\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "hzbtl77TgkmF"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response = chat_completion_with_function_execution(\n",
        "    sql_conversation.conversation_history, functions=functions\n",
        ")\n",
        "try:\n",
        "    assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    print(assistant_message)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print(chat_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoUa3QFJgkv-",
        "outputId": "547e8b06-e6e3-4f66-9746-6139889b959a"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function generation requested, calling function\n",
            "Prepped query is {'query': 'SELECT Artist.Name, COUNT(Track.TrackId) AS TotalTracks FROM Artist JOIN Album ON Artist.ArtistId = Album.ArtistId JOIN Track ON Album.AlbumId = Track.AlbumId GROUP BY Artist.ArtistId ORDER BY TotalTracks DESC LIMIT 5'}\n",
            "楽曲数の多いアーティストトップ5は以下の通りです：\n",
            "\n",
            "1. Iron Maiden - 213曲\n",
            "2. U2 - 135曲\n",
            "3. Led Zeppelin - 114曲\n",
            "4. Metallica - 112曲\n",
            "5. Lost - 92曲\n",
            "\n",
            "これらのアーティストは、Chinook Music Databaseにおいて最も多くの楽曲を持っています。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql_conversation.add_message(\"assistant\", assistant_message)\n",
        "sql_conversation.display_conversation(detailed=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEZ28asxg2Tm",
        "outputId": "eca4ff66-8406-480a-efad-1fd5e46769bd"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system: あなたはChinookGPT、Chinook Music Databaseからユーザーの質問に対する回答を得る、親切なアシスタントです。\n",
            "できるだけ多くの情報をユーザーに提供すること。\n",
            "さあ開始！\n",
            "\n",
            "\n",
            "user: こんにちは、楽曲数の多いアーティストトップ5は誰ですか？\n",
            "\n",
            "\n",
            "function: [('Iron Maiden', 213), ('U2', 135), ('Led Zeppelin', 114), ('Metallica', 112), ('Lost', 92)]\n",
            "\n",
            "\n",
            "assistant: 楽曲数の多いアーティストトップ5は以下の通りです：\n",
            "\n",
            "1. Iron Maiden - 213曲\n",
            "2. U2 - 135曲\n",
            "3. Led Zeppelin - 114曲\n",
            "4. Metallica - 112曲\n",
            "5. Lost - 92曲\n",
            "\n",
            "これらのアーティストは、Chinook Music Databaseにおいて最も多くの楽曲を持っています。\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql_conversation.add_message(\n",
        "    \"user\", \"最も多くの楽曲が収録されているアルバム名を教えてください。\"\n",
        ")"
      ],
      "metadata": {
        "id": "FC-2Br6Lg9at"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response = chat_completion_with_function_execution(\n",
        "    sql_conversation.conversation_history, functions=functions\n",
        ")\n",
        "assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "assistant_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "RxERjHQ6japm",
        "outputId": "c6924ae5-5705-45c0-9b34-079b433f50b7"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function generation requested, calling function\n",
            "Prepped query is {'query': 'SELECT Album.Title, COUNT(Track.TrackId) AS NumTracks FROM Album JOIN Track ON Album.AlbumId = Track.AlbumId GROUP BY Album.Title ORDER BY NumTracks DESC LIMIT 1;'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chinook Music Databaseにおいて、最も多くの楽曲が収録されているアルバムは「Greatest Hits」という名前のアルバムです。このアルバムには57曲が収録されています。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql_conversation.add_message(\"assistant\", assistant_message)"
      ],
      "metadata": {
        "id": "dwdA4ep1j5Ue"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql_conversation.display_conversation(detailed=True)\n",
        "# 実際にはPython上では色分けされているはずです"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXTKFy8yj_Pv",
        "outputId": "30dd482f-52ad-48a3-b8e3-2a043e4ba891"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system: あなたはChinookGPT、Chinook Music Databaseからユーザーの質問に対する回答を得る、親切なアシスタントです。\n",
            "できるだけ多くの情報をユーザーに提供すること。\n",
            "さあ開始！\n",
            "\n",
            "\n",
            "user: こんにちは、楽曲数の多いアーティストトップ5は誰ですか？\n",
            "\n",
            "\n",
            "function: [('Iron Maiden', 213), ('U2', 135), ('Led Zeppelin', 114), ('Metallica', 112), ('Lost', 92)]\n",
            "\n",
            "\n",
            "assistant: 楽曲数の多いアーティストトップ5は以下の通りです：\n",
            "\n",
            "1. Iron Maiden - 213曲\n",
            "2. U2 - 135曲\n",
            "3. Led Zeppelin - 114曲\n",
            "4. Metallica - 112曲\n",
            "5. Lost - 92曲\n",
            "\n",
            "これらのアーティストは、Chinook Music Databaseにおいて最も多くの楽曲を持っています。\n",
            "\n",
            "\n",
            "user: 最も多くの楽曲が収録されているアルバム名を教えてください。\n",
            "\n",
            "\n",
            "function: [('Greatest Hits', 57)]\n",
            "\n",
            "\n",
            "assistant: Chinook Music Databaseにおいて、最も多くの楽曲が収録されているアルバムは「Greatest Hits」という名前のアルバムです。このアルバムには57曲が収録されています。\n",
            "\n",
            "\n",
            "assistant: Chinook Music Databaseにおいて、最も多くの楽曲が収録されているアルバムは「Greatest Hits」という名前のアルバムです。このアルバムには57曲が収録されています。\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 複数の関数の使用\n",
        "\n",
        "では、モデルに複数の関数を呼び出して提供するシナリオを構築してみましょう。ここでは、arXivのデータを使って、学術的なテーマに関する質問に答えるエージェントを作成することにします。このエージェントは、2つの新しい関数を自由に使うことができます：\n",
        "\n",
        "- `get_articles`： get_articles：あるテーマに関するarXivの論文を取得し、リンク付きでユーザーのために要約する関数です。\n",
        "- `read_article_and_summarize`： この関数は、以前に検索された論文の1つを取り、その全体を読み、核となる議論、証拠、結論を要約する。\n",
        "これにより、複数のサービスから選択できる多機能ワークフローに慣れることができ、最初の機能からのデータの一部が2番目の機能で使用されるように永続化されることになります。\n",
        "\n",
        "arXiv検索\n",
        "まず、2つの機能を支えるユーティリティをいくつか設定します。\n",
        "\n",
        "ダウンロードした論文は、ディレクトリ（ここでは ./data/papers を使用）に保存されます。ダウンロードした論文の埋め込みと詳細を保存するために、arxiv_library.csvというファイルを作成し、summarize_textを使って検索するようにします。\n"
      ],
      "metadata": {
        "id": "fu0n8cSHkQrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title mount to drive (ドライブにマウント)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOcfuOHUlZKn",
        "outputId": "b456890b-613e-42e1-e513-cf65932fcfdd"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ダウンロードした論文を保存するディレクトリを設定する\n",
        "# 事前に該当のフォルダ（papersまで）作っておいてください\n",
        "data_dir = os.path.join(os.curdir, \"/content/drive/MyDrive/Colab Notebooks/arXiv\", \"papers\")\n",
        "paper_dir_filepath = \"/content/drive/MyDrive/Colab Notebooks/arXiv/arxiv_library.csv\"\n",
        "\n",
        "# ダウンロードしたファイルを格納するための空白のデータフレームを生成する。\n",
        "df = pd.DataFrame(list())\n",
        "df.to_csv(paper_dir_filepath)"
      ],
      "metadata": {
        "id": "a3Zynq8ykQAm"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def embedding_request(text):\n",
        "    response = openai.Embedding.create(input=text, model=EMBEDDING_MODEL)\n",
        "    return response\n",
        "\n",
        "\n",
        "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
        "    \"\"\"この関数は、ユーザのクエリに基づいて、関連性の高い順に並べられた top_k の記事を取得します。\n",
        "    また、read_article_and_summarizeで取得できるように、ファイルをダウンロードしてarxiv_library.csvに保存します。\n",
        "    \"\"\"\n",
        "    search = arxiv.Search(\n",
        "        query=query, max_results=top_k, sort_by=arxiv.SortCriterion.Relevance\n",
        "    )\n",
        "    result_list = []\n",
        "    for result in search.results():\n",
        "        result_dict = {}\n",
        "        result_dict.update({\"title\": result.title})\n",
        "        result_dict.update({\"summary\": result.summary})\n",
        "\n",
        "        # 最初に提供されたurlを取る\n",
        "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
        "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
        "        result_list.append(result_dict)\n",
        "        print(\"pdf_url:\" + [x.href for x in result.links][1])\n",
        "\n",
        "        # ライブラリファイルにリファレンスを保存する\n",
        "        response = embedding_request(text=result.title)\n",
        "        file_reference = [\n",
        "            result.title,\n",
        "            result.download_pdf(data_dir),\n",
        "            response[\"data\"][0][\"embedding\"],\n",
        "        ]\n",
        "\n",
        "        # ファイルへの書き込み\n",
        "        with open(library, \"a\") as f_object:\n",
        "            writer_object = writer(f_object)\n",
        "            writer_object.writerow(file_reference)\n",
        "            f_object.close()\n",
        "    return result_list"
      ],
      "metadata": {
        "id": "oM39rb66kqSW"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 検索が機能していることをテストする\n",
        "## result_output = get_articles(\"ppo reinforcement learning\")\n",
        "result_output = get_articles(\"computer graphics metaverse\")\n",
        "result_output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMxS4oFoksSm",
        "outputId": "1a19b299-7614-4919-889d-194d9c0381db"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pdf_url:http://arxiv.org/pdf/2205.02764v1\n",
            "pdf_url:http://arxiv.org/pdf/2305.11911v1\n",
            "pdf_url:http://arxiv.org/pdf/2304.13931v1\n",
            "pdf_url:http://arxiv.org/pdf/2208.00369v1\n",
            "pdf_url:http://arxiv.org/pdf/2303.10289v1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Edge-enabled Metaverse: The Convergence of Metaverse and Mobile Edge Computing',\n",
              " 'summary': 'The Metaverse is a virtual environment where users are represented by avatars\\nto navigate a virtual world, which has strong links with the physical one.\\nState-of-the-art Metaverse architectures rely on a cloud-based approach for\\navatar physics emulation and graphics rendering computation. Such centralized\\ndesign is unfavorable as it suffers from several drawbacks caused by the long\\nlatency required for cloud access, such as low quality visualization. To solve\\nthis issue, in this paper, we propose a Fog-Edge hybrid computing architecture\\nfor Metaverse applications that leverage an edge-enabled distributed computing\\nparadigm, which makes use of edge devices computing power to fulfil the\\nrequired computational cost for heavy tasks such as collision detection in\\nvirtual universe and computation of 3D physics in virtual simulation. The\\ncomputational cost related to an entity in the Metaverse such as collision\\ndetection or physics emulation are performed at the end-device of the\\nassociated physical entity. To prove the effectiveness of the proposed\\narchitecture, we simulate a distributed social metaverse application.\\nSimulation results shows that the proposed architecture can reduce the latency\\nby 50% when compared with the legacy cloud-based Metaverse applications.',\n",
              " 'article_url': 'http://arxiv.org/abs/2205.02764v1',\n",
              " 'pdf_url': 'http://arxiv.org/pdf/2205.02764v1'}"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def strings_ranked_by_relatedness(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
        "    top_n: int = 100,\n",
        ") -> list[str]:\n",
        "    \"\"\"文字列とその関連性のリストを、関連性の高いものから低いものへとソートして返す。\"\"\"\n",
        "    query_embedding_response = embedding_request(query)\n",
        "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
        "    strings_and_relatednesses = [\n",
        "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
        "        for i, row in df.iterrows()\n",
        "    ]\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "    return strings[:top_n]"
      ],
      "metadata": {
        "id": "piYSC6tQoVNn"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(filepath):\n",
        "    \"\"\"PDFへのファイルパスを受け取り、PDFの内容の文字列を返す。\"\"\"\n",
        "    # pdfリーダーオブジェクトの作成\n",
        "    reader = PdfReader(filepath)\n",
        "    pdf_text = \"\"\n",
        "    page_number = 0\n",
        "    for page in reader.pages:\n",
        "        page_number += 1\n",
        "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
        "    return pdf_text\n",
        "\n",
        "\n",
        "# テキストをサイズnの小さな塊に分割し、できれば文末で終わるようにする\n",
        "def create_chunks(text, n, tokenizer):\n",
        "    \"\"\"指定されたテキストから連続したn個のチャンクを返します。\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        # 0.5*n個と1.5*n個のトークンの範囲内で、最も近い文末を探す\n",
        "        j = min(i + int(1.5 * n), len(tokens))\n",
        "        while j > i + int(0.5 * n):\n",
        "            # トークンをデコードし、フルストップや改行があるかどうかをチェックする。\n",
        "            chunk = tokenizer.decode(tokens[i:j])\n",
        "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
        "                break\n",
        "            j -= 1\n",
        "        # 文末が見つからない場合、チャンクサイズとしてn個のトークンを使用する\n",
        "        if j == i + int(0.5 * n):\n",
        "            j = min(i + n, len(tokens))\n",
        "        yield tokens[i:j]\n",
        "        i = j\n",
        "\n",
        "\n",
        "def extract_chunk(content, template_prompt):\n",
        "    \"\"\"この関数は、入力されたコンテンツにプロンプトを適用する。この場合、要約されたテキストの塊が返されます。\"\"\"\n",
        "    prompt = template_prompt + content\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def summarize_text(query):\n",
        "    \"\"\"この関数は、以下の処理を行います：\n",
        "    - 埋め込みを含むarxiv_library.csvファイルを読み込む。\n",
        "    - ユーザーのクエリに最も近いファイルを検索します。\n",
        "    - ファイルからテキストを取り出し、チャンク化する。\n",
        "    - 各チャンクを並行して要約する\n",
        "    - 最終的に1つの要約を行い、ユーザーに返す\"\"\"\n",
        "\n",
        "    # 再帰的要約が入力論文にどのようにアプローチすべきかを指示するためのプロンプト\n",
        "    summary_prompt = \"\"\"学術論文からこのテキストを要約しなさい。重要な点があれば、理由をつけて抜き出す\\n\\nContent:\"\"\"\n",
        "\n",
        "    # ライブラリが空の場合（まだ検索が行われていない場合）、検索を行い、結果をダウンロードします\n",
        "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
        "    if len(library_df) == 0:\n",
        "        print(\"まだ検索された論文はない、まずはダウンロードを。\")\n",
        "        get_articles(query)\n",
        "        print(\"論文ダウンロード…継続中\")\n",
        "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
        "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
        "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
        "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
        "    print(\"Chunking text from paper\")\n",
        "    pdf_text = read_pdf(strings[0])\n",
        "\n",
        "    # Initialise tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    results = \"\"\n",
        "\n",
        "    # ドキュメントを1500トークンのチャンクに切り分ける\n",
        "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
        "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
        "    print(\"Summarizing each chunk of text\")\n",
        "\n",
        "    # サマリーを並列処理する\n",
        "    with concurrent.futures.ThreadPoolExecutor(\n",
        "        max_workers=len(text_chunks)\n",
        "    ) as executor:\n",
        "        futures = [\n",
        "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
        "            for chunk in text_chunks\n",
        "        ]\n",
        "        with tqdm(total=len(text_chunks)) as pbar:\n",
        "            for _ in concurrent.futures.as_completed(futures):\n",
        "                pbar.update(1)\n",
        "        for future in futures:\n",
        "            data = future.result()\n",
        "            results += data\n",
        "\n",
        "    # 最終まとめ\n",
        "    print(\"全体まとめにまとめる\")\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=GPT_MODEL,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
        "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.  lang:ja\n",
        "                        User query: {query}\n",
        "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
        "                        Key points:\\n{results}\\nSummary:\\n lang:ja\"\"\",\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response"
      ],
      "metadata": {
        "id": "G4Dzm5kpoYlw"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize_text関数が動作することをテストする\n",
        "## chat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")\n",
        "chat_test_response = summarize_text(\"computer graphics metaverse\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkFpkgWvpYGH",
        "outputId": "9820ee09-9f00-4b0f-8d6f-7e26f332a6e3"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunking text from paper\n",
            "Summarizing each chunk of text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:20<00:00,  4.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "全体まとめにまとめる\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_test_response[\"choices\"][0][\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBwZ9nBkp0OC",
        "outputId": "4f67633e-b0d0-40d5-d0cc-bd83dc8c3f4b"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- この論文は、セマンティックコミュニケーションとAIによるコンテンツ生成を統合するための統一フレームワークについて述べています。\n",
            "- 統合されたセマンティックコミュニケーションとAIによるコンテンツ生成（ISGC）は、セマンティック情報をユーザーの入力から転送し、デジタルコンテンツを生成し、メタバースのグラフィックをレンダリングすることによって注目を集めています。\n",
            "- ISGCは、リソースの最適化された割り当てのための統合ゲインと、目標志向の高品質コンテンツ生成のための調整ゲインの2つの主な利点を提供します。\n",
            "- ISGCは、セマンティック抽出、コンテンツ生成、グラフィックレンダリングのための最適なリソース割り当て戦略を特定するための拡散モデルを使用したケーススタディを提供します。\n",
            "- ISGCの潜在的な応用についても議論されています。\n",
            "- ISGCのフレームワークは、Semantic Module、Inference Module、Rendering Moduleから構成されています。\n",
            "- ISGCの主な利点は、統合による利益であり、シームレスに連携するコンポーネントやリソースを提供します。\n",
            "- ISGCの特徴は、リソースの効率的な利用、柔軟性の向上、統合による利益の提供です。\n",
            "- ISGCの将来の方向性についても議論されており、メタバースでの使用におけるいくつかの課題と方向性が提案されています。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## エージェントの設定\n",
        "ここでは、arXivデータへのアクセスを提供する関数について、2つの関数仕様を作成します。また、Chat Completions API 呼び出しと関数実行を統合するためのユーティリティもいくつか作成する予定です。"
      ],
      "metadata": {
        "id": "uW53NfMeqMEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get_articles と read_article_and_summarize 関数を開始する。\n",
        "arxiv_functions = [\n",
        "    {\n",
        "        \"name\": \"get_articles\",\n",
        "        \"description\": \"\"\"ユーザーの質問に答えるために、arXivから学術論文を取得するために使用する機能です。\"\"\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": f\"\"\"\n",
        "                            User query in JSON. 回答は要約し、記事URLの参照を含める必要があります。\n",
        "                            \"\"\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"],\n",
        "        },\n",
        "        \"name\": \"read_article_and_summarize\",\n",
        "        \"description\": \"\"\"論文全体を読み、ユーザー向けに要約を提供する機能です。\n",
        "        会話でget_articlesが呼び出される前にこの関数を呼び出すことは 絶対に しないでください。\"\"\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": f\"\"\"\n",
        "                            ユーザーのクエリに基づくプレーンテキストでの記事の説明\n",
        "                            \"\"\",\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"query\"],\n",
        "        },\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "kmIHLf0Qr8MQ"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_completion_with_function_execution(messages, functions=[None]):\n",
        "    \"\"\"この関数は、ChatCompletion APIを呼び出すもので、オプションで関数を追加することができます。\"\"\"\n",
        "    response = chat_completion_request(messages, functions)\n",
        "    full_message = response.json()[\"choices\"][0]\n",
        "    if full_message[\"finish_reason\"] == \"function_call\":\n",
        "        print(f\"Function generation requested, calling function\")\n",
        "        return call_arxiv_function(messages, full_message)\n",
        "    else:\n",
        "        print(f\"Function not required, responding to user\")\n",
        "        return response.json()\n",
        "\n",
        "\n",
        "def call_arxiv_function(messages, full_message):\n",
        "    \"\"\"モデルが必要と判断した場合に、関数呼び出しを実行する関数呼び出し機能。\n",
        "    現在は、このif文に節を追加して拡張しています。\"\"\"\n",
        "\n",
        "    if full_message[\"message\"][\"function_call\"][\"name\"] == \"get_articles\":\n",
        "        try:\n",
        "            parsed_output = json.loads(\n",
        "                full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
        "            )\n",
        "            print(\"Getting search results\")\n",
        "            results = get_articles(parsed_output[\"query\"])\n",
        "        except Exception as e:\n",
        "            print(parsed_output)\n",
        "            print(f\"Function execution failed\")\n",
        "            print(f\"Error message: {e}\")\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"function\",\n",
        "                \"name\": full_message[\"message\"][\"function_call\"][\"name\"],\n",
        "                \"content\": str(results),\n",
        "            }\n",
        "        )\n",
        "        try:\n",
        "            print(\"検索結果を取得し、コンテンツを要約する\")\n",
        "            response = chat_completion_request(messages)\n",
        "            return response.json()\n",
        "        except Exception as e:\n",
        "            print(type(e))\n",
        "            raise Exception(\"Function chat request failed\")\n",
        "\n",
        "    elif (\n",
        "        full_message[\"message\"][\"function_call\"][\"name\"] == \"read_article_and_summarize\"\n",
        "    ):\n",
        "        parsed_output = json.loads(\n",
        "            full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
        "        )\n",
        "        print(\"Finding and reading paper\")\n",
        "        summary = summarize_text(parsed_output[\"query\"])\n",
        "        return summary\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"機能が存在しないため、呼び出すことができない\")"
      ],
      "metadata": {
        "id": "dF4afwwWsRbg"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## arXiv conversation\n",
        "Let's test out our function in conversation"
      ],
      "metadata": {
        "id": "N92Ql04Gshh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# システムメッセージから始める\n",
        "paper_system_message = \"\"\"あなたはarXivGPT、ユーザの質問に答えるために、論文を引用するアシスタント。\n",
        "あなたは論文を明確に要約し、ユーザが自分の質問に答えるためにどれを読むべきかを判断できるようにします。\n",
        "ユーザーが論文を理解し、アクセスできるように、常に記事のURLとタイトルを提供します。\n",
        "さあ始めてください！\"\"\"\n",
        "paper_conversation = Conversation()\n",
        "paper_conversation.add_message(\"system\", paper_system_message)\n",
        "# ユーザーメッセージの追加\n",
        "paper_conversation.add_message(\"user\", \"こんにちは、PPO強化学習とはどのようなものなのでしょうか？\")\n",
        "chat_response = chat_completion_with_function_execution(\n",
        "    paper_conversation.conversation_history, functions=arxiv_functions\n",
        ")\n",
        "assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "paper_conversation.add_message(\"assistant\", assistant_message)\n",
        "display(Markdown(assistant_message))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "4YU6_sGJsjkw",
        "outputId": "2b3e59a0-8cb7-43c5-a6a8-67187f19314b"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function generation requested, calling function\n",
            "Finding and reading paper\n",
            "Chunking text from paper\n",
            "Summarizing each chunk of text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:16<00:00,  1.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "全体まとめにまとめる\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-183-b2d23a789fca>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# ユーザーメッセージの追加\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpaper_conversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"こんにちは、PPO強化学習とはどのようなものなのでしょうか？\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m chat_response = chat_completion_with_function_execution(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpaper_conversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconversation_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marxiv_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-182-c59b03496327>\u001b[0m in \u001b[0;36mchat_completion_with_function_execution\u001b[0;34m(messages, functions)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfull_message\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"finish_reason\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"function_call\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Function generation requested, calling function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall_arxiv_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Function not required, responding to user\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-182-c59b03496327>\u001b[0m in \u001b[0;36mcall_arxiv_function\u001b[0;34m(messages, full_message)\u001b[0m\n\u001b[1;32m     48\u001b[0m         )\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finding and reading paper\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-178-992bad6448b9>\u001b[0m in \u001b[0;36msummarize_text\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# 最終まとめ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"全体まとめにまとめる\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGPT_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 5237 tokens. Please reduce the length of the messages."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add another user message to induce our system to use the second tool\n",
        "paper_conversation.add_message(\n",
        "    \"user\",\n",
        "    \"Can you read the PPO sequence generation paper for me and give me a summary\",\n",
        ")\n",
        "updated_response = chat_completion_with_function_execution(\n",
        "    paper_conversation.conversation_history, functions=arxiv_functions\n",
        ")\n",
        "display(Markdown(updated_response[\"choices\"][0][\"message\"][\"content\"]))"
      ],
      "metadata": {
        "id": "Gv1Vw5QYtdqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}